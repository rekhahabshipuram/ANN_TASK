{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55427eeb-2708-4aa9-9855-1f3943cd81f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing /home/rekha-habshipuram/Downloads/archive/train_data/MNIST: [Errno 21] Is a directory: '/home/rekha-habshipuram/Downloads/archive/train_data/MNIST'\n",
      "Error processing /home/rekha-habshipuram/Downloads/archive/val_data/MNIST: [Errno 21] Is a directory: '/home/rekha-habshipuram/Downloads/archive/val_data/MNIST'\n",
      "Error processing /home/rekha-habshipuram/Downloads/archive/test_data/MNIST: [Errno 21] Is a directory: '/home/rekha-habshipuram/Downloads/archive/test_data/MNIST'\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "#implement a perceptron\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features):\n",
    "        self.weights = np.zeros(num_features + 1)  # +1 for the bias\n",
    "        self.learning_rate = 0.1\n",
    "    \n",
    "    def predict(self, features):\n",
    "        # Add bias term (x0 = 1)\n",
    "        activation = np.dot(features, self.weights[1:]) + self.weights[0]\n",
    "        return 1 if activation >= 0 else 0\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        best_val_accuracy = 0\n",
    "        best_weights = np.copy(self.weights)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for features, label in zip(X_train, y_train):\n",
    "                prediction = self.predict(features)\n",
    "                error = label - prediction\n",
    "                self.weights[1:] += self.learning_rate * error * features\n",
    "                self.weights[0] += self.learning_rate * error\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_accuracy = self.evaluate(X_val, y_val)\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_weights = np.copy(self.weights)\n",
    "        \n",
    "        # Set the best weights found during training\n",
    "        self.weights = best_weights\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        correct = 0\n",
    "        for features, label in zip(X, y):\n",
    "            prediction = self.predict(features)\n",
    "            if prediction == label:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(y)\n",
    "        return accuracy\n",
    "\n",
    "def load_and_prepare_data(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Load image and convert to grayscale and then to numpy array\n",
    "            img = Image.open(file_path).convert('L')  # Convert to grayscale\n",
    "            img = img.resize((64, 64))  # Resize image if needed\n",
    "            arr = np.array(img)\n",
    "            \n",
    "            # Flatten array and normalize\n",
    "            features = arr.flatten() / 255.0  # Normalize pixel values\n",
    "            \n",
    "            # Assuming file names are labeled as 'class_name.xxx.jpg'\n",
    "            label = 1 if 'class_name' in file_name else 0\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            continue  # Skip this file and continue with the next one\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "train_folder_path = '/home/rekha-habshipuram/Downloads/archive/train_data'\n",
    "val_folder_path = '/home/rekha-habshipuram/Downloads/archive/val_data'\n",
    "test_folder_path = '/home/rekha-habshipuram/Downloads/archive/test_data'\n",
    "\n",
    "# Load and prepare training, validation, and testing data\n",
    "X_train, y_train = load_and_prepare_data(train_folder_path)\n",
    "X_val, y_val = load_and_prepare_data(val_folder_path)\n",
    "X_test, y_test = load_and_prepare_data(test_folder_path)\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# Initialize and train the perceptron\n",
    "perceptron = Perceptron(num_features=X_train.shape[1])\n",
    "perceptron.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "# Evaluate the perceptron on training, validation, and testing data\n",
    "train_accuracy = perceptron.evaluate(X_train, y_train)\n",
    "val_accuracy = perceptron.evaluate(X_val, y_val)\n",
    "test_accuracy = perceptron.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.0%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb238f11-9949-49f6-9272-1c84cf9c7036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-16 18:10:57.699824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-16 18:10:58.011859: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-16 18:10:58.013979: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-16 18:10:58.454060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-16 18:11:00.338893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rekha-habshipuram/Downloads/archive/train_data/MNIST is not a regular file. Skipping.\n",
      "/home/rekha-habshipuram/Downloads/archive/val_data/MNIST is not a regular file. Skipping.\n",
      "/home/rekha-habshipuram/Downloads/archive/test_data/MNIST is not a regular file. Skipping.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rekha-habshipuram/.local/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7963 - loss: 0.2145 - val_accuracy: 1.0000 - val_loss: 2.0849e-07\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.4205e-08 - val_accuracy: 1.0000 - val_loss: 1.4934e-08\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.2631e-09 - val_accuracy: 1.0000 - val_loss: 6.8307e-09\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 8.3789e-10 - val_accuracy: 1.0000 - val_loss: 5.4218e-09\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.4468e-10 - val_accuracy: 1.0000 - val_loss: 5.0726e-09\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.1931e-10 - val_accuracy: 1.0000 - val_loss: 4.9781e-09\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.8397e-10 - val_accuracy: 1.0000 - val_loss: 4.9519e-09\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.1151e-09 - val_accuracy: 1.0000 - val_loss: 4.9445e-09\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.7835e-10 - val_accuracy: 1.0000 - val_loss: 4.9422e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 4.6071e-10 - val_accuracy: 1.0000 - val_loss: 4.9414e-09\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Training Class Accuracy: 100%\n",
      "Validation Class Accuracy: 100%\n",
      "Testing Class Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "#Build and train a simple neural network using a framework like TensorFlow or PyTorch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_shape):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam',\n",
    "                           loss='binary_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        history = self.model.fit(X_train, y_train,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(X_val, y_val),\n",
    "                                 verbose=1)\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        _, accuracy = self.model.evaluate(X, y, verbose=0)\n",
    "        return accuracy\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
    "\n",
    "def load_and_prepare_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Check if the file is a regular file (not a directory)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    # Load image and convert to grayscale and then to numpy array\n",
    "                    img = Image.open(file_path).convert('L')  # Convert to grayscale\n",
    "                    img = img.resize((64, 64))  # Resize image if needed\n",
    "                    arr = np.array(img)\n",
    "\n",
    "                    # Flatten array and normalize\n",
    "                    features = arr.flatten() / 255.0  # Normalize pixel values\n",
    "\n",
    "                    # Assuming file names are labeled as 'class_name.xxx.jpg'\n",
    "                    label = 1 if 'class_name' in file_name else 0\n",
    "\n",
    "                    X.append(features)\n",
    "                    y.append(label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue  # Skip this file and continue with the next one\n",
    "            else:\n",
    "                print(f\"{file_path} is not a regular file. Skipping.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory '{folder_path}' not found.\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "train_folder_path = '/home/rekha-habshipuram/Downloads/archive/train_data'\n",
    "val_folder_path = '/home/rekha-habshipuram/Downloads/archive/val_data'\n",
    "test_folder_path = '/home/rekha-habshipuram/Downloads/archive/test_data'\n",
    "\n",
    "# Load and prepare training, validation, and testing data\n",
    "X_train, y_train = load_and_prepare_data(train_folder_path)\n",
    "X_val, y_val = load_and_prepare_data(val_folder_path)\n",
    "X_test, y_test = load_and_prepare_data(test_folder_path)\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# Initialize and train the neural network\n",
    "input_shape = X_train.shape[1:]\n",
    "model = SimpleNeuralNetwork(input_shape)\n",
    "history = model.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "# Evaluate the model on training, validation, and testing data\n",
    "train_accuracy = model.evaluate(X_train, y_train)\n",
    "val_accuracy = model.evaluate(X_val, y_val)\n",
    "test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Print accuracies as integers\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.0f}%\")\n",
    "print(f\"Validation Accuracy: {val_accuracy * 100:.0f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.0f}%\")\n",
    "\n",
    "# Individual class accuracies\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "train_class_accuracy = np.mean(y_train == y_train_pred.flatten())\n",
    "val_class_accuracy = np.mean(y_val == y_val_pred.flatten())\n",
    "test_class_accuracy = np.mean(y_test == y_test_pred.flatten())\n",
    "\n",
    "print(f\"Training Class Accuracy: {train_class_accuracy * 100:.0f}%\")\n",
    "print(f\"Validation Class Accuracy: {val_class_accuracy * 100:.0f}%\")\n",
    "print(f\"Testing Class Accuracy: {test_class_accuracy * 100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0b4df4-ac5d-4268-be73-c99aa4b9f9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rekha-habshipuram/Downloads/archive/train_data/MNIST is not a regular file. Skipping.\n",
      "/home/rekha-habshipuram/Downloads/archive/val_data/MNIST is not a regular file. Skipping.\n",
      "/home/rekha-habshipuram/Downloads/archive/test_data/MNIST is not a regular file. Skipping.\n",
      "L1 Regularization:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "L2 Regularization:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "#Experiment with different regularization techniques on a neural network\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, regularization=None, lambda_val=0.01):\n",
    "        self.weights = np.zeros(num_features + 1)  # +1 for the bias\n",
    "        self.learning_rate = 0.1\n",
    "        self.regularization = regularization\n",
    "        self.lambda_val = lambda_val  # Regularization parameter\n",
    "\n",
    "    def predict(self, features):\n",
    "        activation = np.dot(features, self.weights[1:]) + self.weights[0]\n",
    "        return 1 if activation >= 0 else 0\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        best_val_accuracy = 0\n",
    "        best_weights = np.copy(self.weights)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for features, label in zip(X_train, y_train):\n",
    "                prediction = self.predict(features)\n",
    "                error = label - prediction\n",
    "                self.weights[1:] += self.learning_rate * error * features\n",
    "                self.weights[0] += self.learning_rate * error\n",
    "\n",
    "                if self.regularization == 'l1':\n",
    "                    self.weights[1:] -= self.lambda_val * np.sign(self.weights[1:])\n",
    "                elif self.regularization == 'l2':\n",
    "                    self.weights[1:] -= self.lambda_val * self.weights[1:]  # L2 penalty\n",
    "\n",
    "            val_accuracy = self.evaluate(X_val, y_val)\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_weights = np.copy(self.weights)\n",
    "\n",
    "        self.weights = best_weights\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        correct = 0\n",
    "        for features, label in zip(X, y):\n",
    "            prediction = self.predict(features)\n",
    "            if prediction == label:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(y)\n",
    "        return accuracy\n",
    "\n",
    "def load_and_prepare_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    try:\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # Check if the file is a regular file (not a directory)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    # Load image and convert to grayscale and then to numpy array\n",
    "                    img = Image.open(file_path).convert('L')  # Convert to grayscale\n",
    "                    img = img.resize((64, 64))  # Resize image if needed\n",
    "                    arr = np.array(img)\n",
    "\n",
    "                    # Flatten array and normalize\n",
    "                    features = arr.flatten() / 255.0  # Normalize pixel values\n",
    "\n",
    "                    # Assuming file names are labeled as 'class_name.xxx.jpg'\n",
    "                    label = 1 if 'class_name' in file_name else 0\n",
    "\n",
    "                    X.append(features)\n",
    "                    y.append(label)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "                    continue  # Skip this file and continue with the next one\n",
    "            else:\n",
    "                print(f\"{file_path} is not a regular file. Skipping.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory '{folder_path}' not found.\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "train_folder_path = '/home/rekha-habshipuram/Downloads/archive/train_data'\n",
    "val_folder_path = '/home/rekha-habshipuram/Downloads/archive/val_data'\n",
    "test_folder_path = '/home/rekha-habshipuram/Downloads/archive/test_data'\n",
    "\n",
    "# Load and prepare training, validation, and testing data\n",
    "X_train, y_train = load_and_prepare_data(train_folder_path)\n",
    "X_val, y_val = load_and_prepare_data(val_folder_path)\n",
    "X_test, y_test = load_and_prepare_data(test_folder_path)\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# Initialize and train the perceptron with L1 regularization\n",
    "perceptron_l1 = Perceptron(num_features=X_train.shape[1], regularization='l1', lambda_val=0.01)\n",
    "perceptron_l1.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "# Initialize and train the perceptron with L2 regularization\n",
    "perceptron_l2 = Perceptron(num_features=X_train.shape[1], regularization='l2', lambda_val=0.01)\n",
    "perceptron_l2.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "# Evaluate the perceptrons on training, validation, and testing data\n",
    "train_accuracy_l1 = perceptron_l1.evaluate(X_train, y_train)\n",
    "val_accuracy_l1 = perceptron_l1.evaluate(X_val, y_val)\n",
    "test_accuracy_l1 = perceptron_l1.evaluate(X_test, y_test)\n",
    "\n",
    "train_accuracy_l2 = perceptron_l2.evaluate(X_train, y_train)\n",
    "val_accuracy_l2 = perceptron_l2.evaluate(X_val, y_val)\n",
    "test_accuracy_l2 = perceptron_l2.evaluate(X_test, y_test)\n",
    "\n",
    "# Print accuracies as integers\n",
    "print(\"L1 Regularization:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_l1 * 100:.0f}%\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_l1 * 100:.0f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_l1 * 100:.0f}%\")\n",
    "\n",
    "print(\"\\nL2 Regularization:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_l2 * 100:.0f}%\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_l2 * 100:.0f}%\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_l2 * 100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fcb13ad-28f9-4caa-84e6-0d62bb5a51cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with Gradient Descent:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "Performance with Stochastic Gradient Descent:\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n",
      "\n",
      "Performance with Mini-batch Gradient Descent (batch size = 42):\n",
      "Training Accuracy: 100%\n",
      "Validation Accuracy: 100%\n",
      "Testing Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "#Compare performance with various optimization algorithms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features, optimizer='gradient_descent', learning_rate=0.1, batch_size=None):\n",
    "        self.weights = np.zeros(num_features + 1)  # +1 for the bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def predict(self, features):\n",
    "        # Add bias term (x0 = 1)\n",
    "        activation = np.dot(features, self.weights[1:]) + self.weights[0]\n",
    "        return np.where(activation >= 0, 1, 0)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs):\n",
    "        best_val_accuracy = 0\n",
    "        best_weights = np.copy(self.weights)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if self.optimizer == 'gradient_descent':\n",
    "                self.gradient_descent(X_train, y_train)\n",
    "            elif self.optimizer == 'sgd':\n",
    "                self.stochastic_gradient_descent(X_train, y_train)\n",
    "            elif self.optimizer == 'mini_batch':\n",
    "                self.mini_batch_gradient_descent(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on validation set\n",
    "            val_accuracy = self.evaluate(X_val, y_val)\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_weights = np.copy(self.weights)\n",
    "        \n",
    "        # Set the best weights found during training\n",
    "        self.weights = best_weights\n",
    "    \n",
    "    def gradient_descent(self, X_train, y_train):\n",
    "        errors = y_train - self.predict(X_train)\n",
    "        self.weights[1:] += self.learning_rate * np.dot(errors.T, X_train)\n",
    "        self.weights[0] += self.learning_rate * np.sum(errors)\n",
    "    \n",
    "    def stochastic_gradient_descent(self, X_train, y_train):\n",
    "        for features, label in zip(X_train, y_train):\n",
    "            prediction = self.predict(features)\n",
    "            error = label - prediction\n",
    "            self.weights[1:] += self.learning_rate * error * features\n",
    "            self.weights[0] += self.learning_rate * error\n",
    "    \n",
    "    def mini_batch_gradient_descent(self, X_train, y_train):\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = len(X_train)\n",
    "        \n",
    "        for i in range(0, len(X_train), self.batch_size):\n",
    "            X_batch = X_train[i:i+self.batch_size]\n",
    "            y_batch = y_train[i:i+self.batch_size]\n",
    "            \n",
    "            errors = y_batch - self.predict(X_batch)\n",
    "            self.weights[1:] += self.learning_rate * np.dot(errors.T, X_batch)\n",
    "            self.weights[0] += self.learning_rate * np.sum(errors)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        correct = 0\n",
    "        for features, label in zip(X, y):\n",
    "            prediction = self.predict(features)\n",
    "            if prediction == label:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(y)\n",
    "        return accuracy\n",
    "\n",
    "def load_and_prepare_data(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if os.path.isfile(file_path):  # Check if the path is a file\n",
    "            # Load image and convert to grayscale and then to numpy array\n",
    "            img = Image.open(file_path).convert('L')  # Convert to grayscale\n",
    "            img = img.resize((64, 64))  # Resize image if needed\n",
    "            arr = np.array(img)\n",
    "            \n",
    "            # Flatten array and normalize\n",
    "            features = arr.flatten() / 255.0  # Normalize pixel values\n",
    "            \n",
    "            # Assuming file names are labeled as 'class_name.xxx.jpg'\n",
    "            label = 1 if 'class_name' in file_name else 0\n",
    "            \n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Example usage:\n",
    "train_folder_path = '/home/rekha-habshipuram/Downloads/archive/train_data'\n",
    "val_folder_path = '/home/rekha-habshipuram/Downloads/archive/val_data'\n",
    "test_folder_path = '/home/rekha-habshipuram/Downloads/archive/test_data'\n",
    "\n",
    "# Load and prepare training, validation, and testing data\n",
    "X_train, y_train = load_and_prepare_data(train_folder_path)\n",
    "X_val, y_val = load_and_prepare_data(val_folder_path)\n",
    "X_test, y_test = load_and_prepare_data(test_folder_path)\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_index = np.random.permutation(len(X_train))\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# Initialize and train the perceptrons with different optimizers\n",
    "perceptron_gd = Perceptron(num_features=X_train.shape[1], optimizer='gradient_descent', learning_rate=0.1)\n",
    "perceptron_gd.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "perceptron_sgd = Perceptron(num_features=X_train.shape[1], optimizer='sgd', learning_rate=0.1)\n",
    "perceptron_sgd.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "perceptron_mini_batch = Perceptron(num_features=X_train.shape[1], optimizer='mini_batch', learning_rate=0.1, batch_size=32)\n",
    "perceptron_mini_batch.train(X_train, y_train, X_val, y_val, epochs=10)\n",
    "\n",
    "# Evaluate the perceptrons on training, validation, and testing data\n",
    "train_accuracy_gd = perceptron_gd.evaluate(X_train, y_train)\n",
    "val_accuracy_gd = perceptron_gd.evaluate(X_val, y_val)\n",
    "test_accuracy_gd = perceptron_gd.evaluate(X_test, y_test)\n",
    "\n",
    "train_accuracy_sgd = perceptron_sgd.evaluate(X_train, y_train)\n",
    "val_accuracy_sgd = perceptron_sgd.evaluate(X_val, y_val)\n",
    "test_accuracy_sgd = perceptron_sgd.evaluate(X_test, y_test)\n",
    "\n",
    "train_accuracy_mini_batch = perceptron_mini_batch.evaluate(X_train, y_train)\n",
    "val_accuracy_mini_batch = perceptron_mini_batch.evaluate(X_val, y_val)\n",
    "test_accuracy_mini_batch = perceptron_mini_batch.evaluate(X_test, y_test)\n",
    "\n",
    "# Print results\n",
    "print(\"Performance with Gradient Descent:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_gd:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_gd:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_gd:.0%}\")\n",
    "\n",
    "print(\"\\nPerformance with Stochastic Gradient Descent:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_sgd:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_sgd:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_sgd:.0%}\")\n",
    "\n",
    "print(\"\\nPerformance with Mini-batch Gradient Descent (batch size = 42):\")\n",
    "print(f\"Training Accuracy: {train_accuracy_mini_batch:.0%}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy_mini_batch:.0%}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy_mini_batch:.0%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b07f8ec-1e1f-48fc-a574-7f92f56a1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4266958488757735\n",
      "Epoch 2, Loss: 0.17506469418210244\n",
      "Epoch 3, Loss: 0.13070789829946125\n",
      "Epoch 4, Loss: 0.10558524214649691\n",
      "Epoch 5, Loss: 0.08751164211865749\n",
      "Epoch 6, Loss: 0.07552732143271813\n",
      "Epoch 7, Loss: 0.06824164294285885\n",
      "Epoch 8, Loss: 0.05799117223150917\n",
      "Epoch 9, Loss: 0.054232563490864076\n",
      "Epoch 10, Loss: 0.0461434526472097\n",
      "Training Accuracy: 98.91833333333334 %\n",
      "Test Accuracy: 97.67 %\n",
      "Validation Accuracy: 97.67 %\n"
     ]
    }
   ],
   "source": [
    "#Create a multi-layer perceptron (MLP) for digit classification (MNIST dataset)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST('/home/rekha-habshipuram/Downloads/archive/train_data', download=True, train=True, transform=transform)\n",
    "validation_data = datasets.MNIST('/home/rekha-habshipuram/Downloads/archive/test_dat', download=True, train=False, transform=transform)\n",
    "test_data = datasets.MNIST('/home/rekha-habshipuram/Downloads/archive/val_data', download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # input layer (28x28) -> hidden layer (128)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),  # hidden layer (128) -> hidden layer (64)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)  # hidden layer (64) -> output layer (10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/i}')\n",
    "\n",
    "# Evaluate on training set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8824306-99ae-4609-ad25-ea7404c2672e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
